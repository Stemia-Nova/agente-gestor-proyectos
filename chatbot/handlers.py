#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
chatbot/handlers.py
-------------------
Versi√≥n refinada para integraci√≥n con `chatbot/prompts.py`.

‚úî Integra HybridSearch (RAG)
‚úî Usa prompts especializados (Scrum/Agile)
‚úî Genera respuestas naturales o JSON seg√∫n el tipo de consulta
‚úî Incluye memoria conversacional, comandos y debug
‚úî Compatible con Chainlit 2.8.x y `main.py` cl√°sico
"""

import os
import asyncio
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from dotenv import load_dotenv
from openai import OpenAI
from utils.hybrid_search import HybridSearch
from chatbot import prompts  # importamos tu prompts.py

# ======================================================
# ‚öôÔ∏è CARGA DE ENTORNO
# ======================================================
load_dotenv()

MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
REQUEST_TIMEOUT = float(os.getenv("OPENAI_REQUEST_TIMEOUT", 60))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

try:
    client: Optional[OpenAI] = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
except Exception as e:
    print(f"‚ö†Ô∏è No se pudo inicializar OpenAI: {e}")
    client = None

# Instanciar HybridSearch (sin par√°metros, seg√∫n tu clase actual)
hybrid_search = HybridSearch()

# ======================================================
# üíæ MEMORIA CONVERSACIONAL
# ======================================================
_conversation_history: List[Dict[str, str]] = []


def _log_conversation(q: str, r: str) -> None:
    """Guarda en memoria las √∫ltimas interacciones."""
    _conversation_history.append(
        {"timestamp": datetime.now().isoformat(timespec="seconds"), "question": q, "answer": r}
    )
    if len(_conversation_history) > 5:
        _conversation_history.pop(0)


def reset_memory() -> str:
    _conversation_history.clear()
    return "üßπ Memoria conversacional reiniciada."


# ======================================================
# üßÆ UTILIDADES DE FORMATEO
# ======================================================

def summarize_context(meta: List[Dict[str, Any]]) -> str:
    """Crea un resumen textual de las tareas recuperadas."""
    if not meta:
        return "(sin contexto)"
    resumen = []
    for m in meta[:5]:
        resumen.append(
            f"- {m.get('name','Tarea sin nombre')} "
            f"(Sprint {m.get('sprint','?')}) ‚Äî "
            f"{m.get('status','?')}, "
            f"{m.get('assignees','Sin asignar')}, "
            f"prioridad: {m.get('priority','Sin prioridad')}."
        )
    return "\n".join(resumen)


# ======================================================
# üß† GENERACI√ìN CON OPENAI
# ======================================================

async def _synthesize_openai(question: str, context: str) -> str:
    """Genera respuesta contextual con OpenAI o fallback."""
    context = str(context or "")
    if not client:
        return prompts.DEFAULT_ECHO_PREFIX + " " + context if context.strip() else prompts.RAG_NO_RESULTS

    # Construimos prompt contextual
    user_prompt = prompts.RAG_CONTEXT_PROMPT.format(
        system=prompts.SYSTEM_INSTRUCTIONS,
        context=context,
        question=question,
    )

    # Intentamos hasta 3 veces (por rate limit)
    for attempt in range(3):
        try:
            completion = await asyncio.to_thread(
                lambda: client.chat.completions.create(
                    model=MODEL,
                    messages=[
                        {"role": "system", "content": prompts.SYSTEM_INSTRUCTIONS},
                        {"role": "user", "content": user_prompt},
                    ],
                    temperature=0.25,
                    max_tokens=450,
                    timeout=REQUEST_TIMEOUT,
                )
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            if "429" in str(e):
                delay = (attempt + 1) * 2
                print(f"‚ö†Ô∏è Rate limit alcanzado. Reintentando en {delay}s...")
                time.sleep(delay)
                continue
            print(f"‚ö†Ô∏è Error con OpenAI: {e}")
            break

    # Fallback textual
    return (
        "Respuesta basada en el contexto:\n" + context
        if context.strip()
        else prompts.RAG_NO_RESULTS
    )


# ======================================================
# üí¨ MANEJADOR PRINCIPAL
# ======================================================

async def handle_query(query: str) -> str:
    """Maneja consultas del usuario con RAG, prompts y memoria."""
    q = (query or "").strip()
    if not q:
        return "Por favor, escribe una pregunta."

    # --- Comandos ---
    if q.lower() in {"/ayuda", "ayuda"}:
        return (
            "üìò Comandos disponibles:\n"
            "‚Ä¢ /contexto ‚Üí muestra las √∫ltimas interacciones.\n"
            "‚Ä¢ /reset ‚Üí borra la memoria conversacional.\n"
            "‚Ä¢ /debug ‚Üí muestra el √∫ltimo prompt enviado al modelo.\n"
            "‚Ä¢ /ayuda ‚Üí muestra esta lista.\n"
        )

    if q.lower() in {"/reset", "reset"}:
        return reset_memory()

    if q.lower() in {"/contexto", "contexto"}:
        if not _conversation_history:
            return "üß† Memoria vac√≠a."
        texto = "\n\n".join(
            f"[{x['timestamp']}] Q: {x['question']}\nA: {x['answer']}"
            for x in _conversation_history
        )
        return f"üß† Memoria reciente:\n{texto}"

    # --- B√∫squeda h√≠brida ---
    try:
        results, metas = hybrid_search.search(q, top_k=5)  # type: ignore[attr-defined]
    except Exception as e:
        r = f"‚ö†Ô∏è Error en la b√∫squeda: {e}"
        _log_conversation(q, r)
        return r

    if not results:
        r = prompts.RAG_NO_RESULTS
        _log_conversation(q, r)
        return r

    # --- Preparar contexto y generar respuesta ---
    context_text = summarize_context(metas)
    answer = await _synthesize_openai(q, context_text)

    # --- Guardar en memoria y devolver ---
    _log_conversation(q, answer)
    return answer


# ======================================================
# üß© DEBUG LOCAL
# ======================================================

if __name__ == "__main__":
    import asyncio

    print("ü§ñ Prueba manual del handler con prompts √°giles")
    res = asyncio.run(handle_query("¬øQu√© tareas est√°n bloqueadas?"))
    print(res)
