#!/usr/bin/env python3
"""
Test para validar el conteo de sprints con enfoque hÃ­brido profesional.

Problema reportado: "cuÃ¡ntos sprints hay?" retornaba 24 (total de tareas)
SoluciÃ³n: Enfoque hÃ­brido - delegar al LLM con contexto enriquecido (flexible + inteligente)
"""

import sys
sys.path.insert(0, '/home/st12/agente-gestor-proyectos/agente-gestor-proyectos')

from utils.hybrid_search import HybridSearch
from dotenv import load_dotenv
import time

# Cargar variables de entorno
load_dotenv()

def verificar_chromadb():
    """Verificar datos en ChromaDB."""
    print("=" * 70)
    print("ğŸ” VERIFICACIÃ“N DE CHROMADB")
    print("=" * 70)
    
    import chromadb
    
    client = chromadb.PersistentClient(path="data/rag/chroma_db")
    collection = client.get_or_create_collection(name="clickup_tasks")
    result = collection.get(include=['metadatas'])
    
    metas = result['metadatas']
    
    # Extraer sprints Ãºnicos
    sprints = set()
    sprint_counts = {}
    
    for m in metas:
        sprint_name = m.get('sprint')
        if sprint_name:
            sprints.add(sprint_name)
            sprint_counts[sprint_name] = sprint_counts.get(sprint_name, 0) + 1
    
    print(f"\nTotal tareas: {len(metas)}")
    print(f"Sprints Ãºnicos: {len(sprints)}")
    print("\nDistribuciÃ³n:")
    for sprint in sorted(sprints):
        count = sprint_counts.get(sprint, 0)
        print(f"  â€¢ {sprint}: {count} tareas")
    
    print("=" * 70)
    return len(sprints), sprint_counts

def test_conteo_sprints_hibrido():
    """Test del conteo de sprints con enfoque hÃ­brido (LLM)."""
    print("\n" + "=" * 70)
    print("ğŸ§ª TEST: Conteo de Sprints (Enfoque HÃ­brido - LLM)")
    print("=" * 70)
    
    hs = HybridSearch(collection_name="clickup_tasks")
    
    # Variantes de la pregunta (incluyendo reformulaciones)
    queries = [
        "Â¿cuÃ¡ntos sprints hay?",
        "cuantos sprints hay",
        "nÃºmero de sprints en el proyecto",
        "cuÃ¡ntas iteraciones tenemos",
        "how many sprints",  # inglÃ©s
    ]
    
    print("\nğŸ“ Probando variantes de la pregunta:")
    print("   (El LLM debe entender todas las reformulaciones)\n")
    
    results = []
    for i, query in enumerate(queries, 1):
        print(f"{i}. Query: \"{query}\"")
        
        # Medir tiempo
        start = time.time()
        response = hs.answer(query, temperature=0.2)  # Usar answer() que delega al LLM
        elapsed = time.time() - start
        
        print(f"   Tiempo: {elapsed:.2f}s")
        print(f"   Respuesta: {response}")
        
        # Validar respuesta
        if "3" in response and ("sprint" in response.lower() or "iteraci" in response.lower()):
            print("   âœ… CORRECTO: Detecta 3 sprints")
            results.append(True)
        elif "24" in response and "tarea" in response.lower():
            print("   âŒ ERROR: EstÃ¡ contando tareas (24) en vez de sprints")
            results.append(False)
        else:
            print("   âš ï¸  Respuesta inesperada (verificar manualmente)")
            results.append(None)
        
        print()
    
    # Resumen
    print("=" * 70)
    passed = sum(1 for r in results if r is True)
    failed = sum(1 for r in results if r is False)
    
    if failed > 0:
        print(f"âŒ TEST FALLIDO: {failed}/{len(queries)} respuestas incorrectas")
        return False
    elif passed == len(queries):
        print(f"âœ… TEST PASADO: {passed}/{len(queries)} respuestas correctas")
        print("\nğŸ’¡ VENTAJAS DEL ENFOQUE HÃBRIDO:")
        print("   â€¢ Entiende reformulaciones naturales")
        print("   â€¢ Funciona en mÃºltiples idiomas")
        print("   â€¢ No requiere regex por cada variante")
        print("   â€¢ Proporciona contexto adicional (distribuciÃ³n)")
        return True
    else:
        print(f"âš ï¸  TEST PARCIAL: {passed}/{len(queries)} correctas, {len(queries) - passed - failed} inconclusas")
        return None

def test_comparacion_enfoques():
    """Demostrar superioridad del enfoque hÃ­brido vs manual."""
    print("\n" + "=" * 70)
    print("ğŸ“Š COMPARACIÃ“N: Enfoque Manual vs HÃ­brido (LLM)")
    print("=" * 70)
    
    queries_dificiles = [
        ("Â¿En cuÃ¡ntos ciclos dividieron el trabajo?", "Requiere sinÃ³nimo (ciclo=sprint)"),
        ("NÃºmero de iteraciones en el proyecto", "ReformulaciÃ³n tÃ©cnica"),
        ("Quiero saber cuÃ¡ntos sprints han creado", "FormulaciÃ³n indirecta"),
        ("how many sprints are there?", "InglÃ©s"),
    ]
    
    print("\nğŸ”§ ENFOQUE MANUAL (Regex):")
    print("   âŒ Requiere regex por cada variante")
    print("   âŒ No entiende sinÃ³nimos")
    print("   âŒ Un idioma por implementaciÃ³n")
    print("   âœ… RÃ¡pido (sin latencia LLM)")
    print("   âœ… DeterminÃ­stico")
    
    print("\nğŸ§  ENFOQUE HÃBRIDO (LLM):")
    print("   âœ… Entiende reformulaciones naturales")
    print("   âœ… Multiidioma sin cambios")
    print("   âœ… Contexto enriquecido (distribuciÃ³n)")
    print("   âš ï¸  Latencia ~1-2s (aceptable para UX)")
    print("   âš ï¸  Costo por query (~$0.0001)")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ CONCLUSIÃ“N: HÃ­brido es MÃS PROFESIONAL para casos no crÃ­ticos")
    print("=" * 70)

if __name__ == "__main__":
    # 1. Verificar ChromaDB
    num_sprints, sprint_counts = verificar_chromadb()
    
    # 2. Ejecutar test principal
    success = test_conteo_sprints_hibrido()
    
    # 3. Mostrar comparaciÃ³n
    test_comparacion_enfoques()
    
    # 4. Resultado final
    if success:
        print("\nğŸ‰ Â¡Test completado exitosamente!")
        print(f"\nâœ… El sistema detecta correctamente {num_sprints} sprints Ãºnicos")
        print("âœ… El enfoque hÃ­brido (LLM) proporciona flexibilidad profesional")
        sys.exit(0)
    elif success is False:
        print("\nâŒ Test fallÃ³")
        sys.exit(1)
    else:
        print("\nâš ï¸  Test inconcluso - revisar manualmente")
        sys.exit(2)
