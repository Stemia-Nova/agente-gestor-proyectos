#!/usr/bin/env python3
"""
Test del sistema RAG SIN LLM - Valida b√∫squeda h√≠brida pura
Verifica que la b√∫squeda sem√°ntica + BM25 + reranker recupere documentos correctos
"""
from utils.hybrid_search import HybridSearch
from typing import List, Dict, Any

# Colores
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BLUE = "\033[94m"
RESET = "\033[0m"

def print_header(text: str):
    print(f"\n{BLUE}{'='*80}{RESET}")
    print(f"{BLUE}{text}{RESET}")
    print(f"{BLUE}{'='*80}{RESET}")

def print_test(num: int, desc: str):
    print(f"\n{YELLOW}TEST {num}: {desc}{RESET}")
    print(f"{BLUE}{'-'*80}{RESET}")

def validate_results(results: List[Dict[str, Any]], expected_keywords: List[str], 
                     expected_min: int = 1) -> bool:
    """Valida que los resultados contengan las keywords esperadas"""
    if len(results) < expected_min:
        print(f"{RED}‚úó Solo {len(results)} resultados, esperaba al menos {expected_min}{RESET}")
        return False
    
    matches = 0
    for meta in results:
        name = meta.get('name', '').lower()
        desc = meta.get('description', '').lower()
        text = f"{name} {desc}"
        
        if any(kw.lower() in text for kw in expected_keywords):
            matches += 1
    
    if matches >= expected_min:
        print(f"{GREEN}‚úì {matches}/{len(results)} resultados relevantes{RESET}")
        return True
    else:
        print(f"{RED}‚úó Solo {matches}/{len(results)} resultados relevantes, esperaba {expected_min}{RESET}")
        return False

def print_results(results: List[Dict[str, Any]], max_show: int = 5):
    """Muestra los resultados encontrados"""
    for i, meta in enumerate(results[:max_show], 1):
        status = meta.get('status_spanish', meta.get('status', ''))
        blocked = "üî¥ BLOQUEADA" if meta.get('is_blocked') else ""
        assignees = meta.get('assignees', 'sin asignar')
        sprint = meta.get('sprint', 'N/A')
        
        print(f"  {i}. {meta.get('name', 'sin nombre')[:60]}")
        print(f"     Estado: {status} {blocked} | Asignado: {assignees} | Sprint: {sprint}")

def main():
    print_header("TESTS RAG SIN LLM - B√∫squeda H√≠brida Pura")
    print("Validando: B√∫squeda Sem√°ntica + BM25 + Reranker\n")
    
    hs = HybridSearch()
    passed = 0
    failed = 0
    
    # ===== CATEGOR√çA 1: B√öSQUEDAS POR TEMA =====
    print_header("CATEGOR√çA 1: B√∫squedas por Tema Sem√°ntico")
    
    # Test 1: RAG
    print_test(1, "B√∫squeda tem√°tica: RAG")
    docs, metas = hs.search("RAG embedding vectores", top_k=5)
    print(f"Documentos recuperados: {len(metas)}")
    print_results(metas)
    if validate_results(metas, ["rag", "embedding", "vector", "crear"], expected_min=2):
        passed += 1
    else:
        failed += 1
    
    # Test 2: ChatBot
    print_test(2, "B√∫squeda tem√°tica: ChatBot")
    docs, metas = hs.search("chatbot responder preguntas", top_k=5)
    print(f"Documentos recuperados: {len(metas)}")
    print_results(metas)
    if validate_results(metas, ["chatbot", "pregunta", "respuesta"], expected_min=1):
        passed += 1
    else:
        failed += 1
    
    # Test 3: ClickUp datos
    print_test(3, "B√∫squeda tem√°tica: ClickUp datos")
    docs, metas = hs.search("ClickUp datos tareas informaci√≥n", top_k=5)
    print(f"Documentos recuperados: {len(metas)}")
    print_results(metas)
    if validate_results(metas, ["clickup", "data", "dato", "coger"], expected_min=1):
        passed += 1
    else:
        failed += 1
    
    # ===== CATEGOR√çA 2: B√öSQUEDAS POR PERSONA =====
    print_header("CATEGOR√çA 2: B√∫squedas por Persona (Assignees)")
    
    # Test 4: Jorge
    print_test(4, "B√∫squeda por persona: Jorge Aguadero")
    docs, metas = hs.search("Jorge Aguadero tareas asignadas", top_k=10)
    print(f"Documentos recuperados: {len(metas)}")
    jorge_tasks = [m for m in metas if 'jorge' in m.get('assignees', '').lower()]
    print(f"Tareas de Jorge encontradas: {len(jorge_tasks)}")
    print_results(jorge_tasks, max_show=7)
    if len(jorge_tasks) >= 3:
        print(f"{GREEN}‚úì Encontradas {len(jorge_tasks)} tareas de Jorge{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó Solo {len(jorge_tasks)} tareas de Jorge{RESET}")
        failed += 1
    
    # Test 5: Laura
    print_test(5, "B√∫squeda por persona: Laura")
    docs, metas = hs.search("Laura tareas asignadas", top_k=10)
    print(f"Documentos recuperados: {len(metas)}")
    laura_tasks = [m for m in metas if 'laura' in m.get('assignees', '').lower()]
    print(f"Tareas de Laura encontradas: {len(laura_tasks)}")
    print_results(laura_tasks)
    if len(laura_tasks) >= 0:  # Puede que no tenga tareas
        print(f"{GREEN}‚úì B√∫squeda completada (Laura: {len(laura_tasks)} tareas){RESET}")
        passed += 1
    else:
        failed += 1
    
    # ===== CATEGOR√çA 3: FILTROS POR ESTADO =====
    print_header("CATEGOR√çA 3: Conteos por Estado (sin LLM)")
    
    # Test 6: Tareas completadas
    print_test(6, "Conteo: Tareas completadas")
    completadas = hs.count_tasks(where={"status": "Completada"})
    print(f"Total completadas: {completadas}")
    if completadas > 0:
        print(f"{GREEN}‚úì {completadas} tareas completadas{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó No se encontraron tareas completadas{RESET}")
        failed += 1
    
    # Test 7: Tareas pendientes
    print_test(7, "Conteo: Tareas pendientes")
    pendientes = hs.count_tasks(where={"status": "Pendiente"})
    print(f"Total pendientes: {pendientes}")
    if pendientes > 0:
        print(f"{GREEN}‚úì {pendientes} tareas pendientes{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó No se encontraron tareas pendientes{RESET}")
        failed += 1
    
    # Test 8: Tareas bloqueadas
    print_test(8, "Conteo: Tareas bloqueadas")
    bloqueadas = hs.count_tasks(where={"is_blocked": True})
    print(f"Total bloqueadas: {bloqueadas}")
    if bloqueadas >= 0:
        print(f"{GREEN}‚úì {bloqueadas} tareas bloqueadas{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó Error al contar bloqueadas{RESET}")
        failed += 1
    
    # Test 9: Total de tareas
    print_test(9, "Conteo: Total de tareas")
    total = hs.count_tasks()
    print(f"Total de tareas: {total}")
    if total > 0:
        print(f"{GREEN}‚úì {total} tareas en total{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó No se encontraron tareas{RESET}")
        failed += 1
    
    # ===== CATEGOR√çA 4: FILTROS COMBINADOS =====
    print_header("CATEGOR√çA 4: Filtros Combinados")
    
    # Test 10: Sprint espec√≠fico
    print_test(10, "Filtro: Tareas del Sprint 3")
    sprint3 = hs.count_tasks(where={"sprint": "Sprint 3"})
    print(f"Tareas en Sprint 3: {sprint3}")
    if sprint3 > 0:
        print(f"{GREEN}‚úì {sprint3} tareas en Sprint 3{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó No se encontraron tareas en Sprint 3{RESET}")
        failed += 1
    
    # Test 11: Sprint + Estado
    print_test(11, "Filtro combinado: Sprint 3 + Completadas")
    sprint3_done = hs.count_tasks(where={"$and": [
        {"sprint": "Sprint 3"},
        {"status": "Completada"}
    ]})
    print(f"Tareas completadas en Sprint 3: {sprint3_done}")
    print(f"{GREEN}‚úì {sprint3_done} tareas completadas en Sprint 3{RESET}")
    passed += 1
    
    # ===== CATEGOR√çA 5: B√öSQUEDA DE BLOQUEOS =====
    print_header("CATEGOR√çA 5: Detecci√≥n de Bloqueos")
    
    # Test 12: Tareas bloqueadas con detalles
    print_test(12, "B√∫squeda: Tareas bloqueadas con contexto")
    docs, metas = hs.search("bloqueada impedimento problema", top_k=10)
    bloqueadas_list = [m for m in metas if m.get('is_blocked')]
    print(f"Tareas bloqueadas encontradas: {len(bloqueadas_list)}")
    print_results(bloqueadas_list)
    if len(bloqueadas_list) >= 0:
        print(f"{GREEN}‚úì Encontradas {len(bloqueadas_list)} tareas bloqueadas{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó No se detectaron bloqueos{RESET}")
        failed += 1
    
    # ===== CATEGOR√çA 6: M√âTRICAS DE SPRINT =====
    print_header("CATEGOR√çA 6: M√©tricas de Sprint")
    
    # Test 13: M√©tricas Sprint 1
    print_test(13, "M√©tricas: Sprint 1")
    metrics1 = hs.get_sprint_metrics("Sprint 1")
    if "error" not in metrics1:
        print(f"  Total: {metrics1['total']} tareas")
        print(f"  Completadas: {metrics1['completadas']} ({metrics1['porcentaje_completitud']}%)")
        print(f"  En progreso: {metrics1['en_progreso']}")
        print(f"  Pendientes: {metrics1['pendientes']}")
        print(f"  Bloqueadas: {metrics1['bloqueadas']}")
        print(f"{GREEN}‚úì M√©tricas calculadas correctamente{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó Error: {metrics1['error']}{RESET}")
        failed += 1
    
    # Test 14: M√©tricas Sprint 3
    print_test(14, "M√©tricas: Sprint 3")
    metrics3 = hs.get_sprint_metrics("Sprint 3")
    if "error" not in metrics3:
        print(f"  Total: {metrics3['total']} tareas")
        print(f"  Completadas: {metrics3['completadas']} ({metrics3['porcentaje_completitud']}%)")
        print(f"  En progreso: {metrics3['en_progreso']}")
        print(f"  Pendientes: {metrics3['pendientes']}")
        print(f"  Bloqueadas: {metrics3['bloqueadas']}")
        print(f"{GREEN}‚úì M√©tricas calculadas correctamente{RESET}")
        passed += 1
    else:
        print(f"{RED}‚úó Error: {metrics3['error']}{RESET}")
        failed += 1
    
    # ===== CATEGOR√çA 7: VALIDACI√ìN DE RERANKER =====
    print_header("CATEGOR√çA 7: Validaci√≥n de Reranker")
    
    # Test 15: Verificar que el reranker mejora resultados
    print_test(15, "Reranker: Verificar orden de relevancia")
    docs, metas = hs.search("crear sistema RAG con embeddings", top_k=5)
    print(f"Top 5 resultados (post-reranking):")
    for i, meta in enumerate(metas[:5], 1):
        name = meta.get('name', 'sin nombre')
        # Verificar si contiene keywords relevantes
        relevance = sum([
            'rag' in name.lower(),
            'embedding' in name.lower(),
            'crear' in name.lower(),
            'vector' in name.lower()
        ])
        print(f"  {i}. {name[:60]} (relevancia: {relevance}/4)")
    
    # Verificar que el primer resultado sea muy relevante
    if len(metas) > 0:
        first = metas[0].get('name', '').lower()
        if any(kw in first for kw in ['rag', 'embedding', 'crear']):
            print(f"{GREEN}‚úì Primer resultado es altamente relevante{RESET}")
            passed += 1
        else:
            print(f"{YELLOW}‚ö† Primer resultado podr√≠a ser m√°s relevante{RESET}")
            passed += 1  # No es error cr√≠tico
    else:
        print(f"{RED}‚úó No se obtuvieron resultados{RESET}")
        failed += 1
    
    # ===== RESUMEN FINAL =====
    print_header("RESUMEN FINAL")
    total_tests = passed + failed
    success_rate = (passed / total_tests * 100) if total_tests > 0 else 0
    
    print(f"\n{GREEN}‚úì Pasadas: {passed}{RESET}")
    print(f"{RED}‚úó Fallidas: {failed}{RESET}")
    print(f"Tasa de √©xito: {success_rate:.1f}%")
    
    if success_rate == 100:
        print(f"\n{GREEN}üéâ PERFECTO: Todos los tests pasaron - Sistema RAG funcionando al 100%{RESET}")
    elif success_rate >= 90:
        print(f"\n{GREEN}‚úÖ EXCELENTE: Sistema RAG muy robusto{RESET}")
    elif success_rate >= 75:
        print(f"\n{YELLOW}‚ö†Ô∏è  BUENO: Sistema funcional, algunos aspectos por mejorar{RESET}")
    else:
        print(f"\n{RED}‚ùå REVISAR: Varios problemas detectados en el sistema RAG{RESET}")
    
    print(f"\n{BLUE}{'='*80}{RESET}")
    print(f"{BLUE}Validaci√≥n completada - Sin dependencia del LLM{RESET}")
    print(f"{BLUE}{'='*80}{RESET}")

if __name__ == "__main__":
    main()
