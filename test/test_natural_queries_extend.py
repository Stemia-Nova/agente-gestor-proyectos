#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TEST EXTENDIDO DE PREGUNTAS NATURALES â€” v3
--------------------------------------------------
EvalÃºa la comprensiÃ³n y coherencia del agente gestor de proyectos.
"""

import sys
import asyncio
from pathlib import Path
import re
import pytest
from chatbot import handlers

sys.path.append(str(Path(__file__).resolve().parents[1]))

# =============================================================
# ğŸ” BaterÃ­a ampliada de consultas naturales
# =============================================================
NATURAL_QUERIES = [
    ("Â¿QuÃ© tareas estÃ¡n bloqueadas ahora mismo?", ["bloquead"]),
    ("Â¿Hay alguna tarea pendiente?", ["pendient", "curso", "in progress"]),
    ("Â¿CuÃ¡ntas tareas hay completadas?", ["complet", "finaliz"]),
    ("Â¿CuÃ¡ntas tareas tiene asignadas Laura?", ["laura"]),
    ("Â¿QuÃ© tareas tiene Jorge en curso?", ["jorge", "curso"]),
    ("Â¿CuÃ¡ntos sprint hemos tenido hasta ahora?", ["sprint"]),
    ("Â¿QuÃ© tareas pertenecen al Sprint 2?", ["sprint", "2"]),
    ("Â¿QuÃ© tareas son urgentes?", ["urgente", "alta"]),
    ("MuÃ©strame las tareas sin prioridad", ["sin prioridad", "normal"]),
    ("Â¿QuÃ© tareas estÃ¡n esperando revisiÃ³n?", ["revisiÃ³n", "review"]),
    ("Â¿Hay alguna tarea asignada pero sin empezar?", ["asignad", "pendient"]),
    ("Â¿QuÃ© tareas son de Laura pero estÃ¡n bloqueadas?", ["laura", "bloquead"]),
    ("Â¿CuÃ¡ntas tareas estÃ¡n en curso en el Sprint 3?", ["curso", "sprint", "3"]),
    ("Â¿QuÃ© tareas estÃ¡n completadas y a quiÃ©n se asignaron?", ["complet", "asign"]),
    ("Â¿CuÃ¡les son las tareas crÃ­ticas del proyecto?", ["urgente", "alta", "prioridad"]),
    ("Â¿QuÃ© tareas faltan por terminar?", ["pendient", "curso", "todo"]),
    ("Â¿QuiÃ©n tiene mÃ¡s tareas asignadas?", ["jorge", "laura", "asign"]),
    ("Â¿CuÃ¡ntas tareas bloqueadas hay por sprint?", ["bloquead", "sprint"]),
    ("Â¿QuÃ© tareas pertenecen al Sprint actual?", ["sprint"]),
    ("Â¿CuÃ¡ntas tareas hay en total en el proyecto?", ["total", "tarea"]),
]

# =============================================================
# ğŸ§© Funciones auxiliares
# =============================================================
def _has_keywords(text: str, expected_keywords: list[str]) -> bool:
    lower = text.lower()
    return any(k in lower for k in expected_keywords)

def _is_meaningful(text: str) -> bool:
    """Descarta respuestas vacÃ­as o genÃ©ricas."""
    lower = text.lower()
    if not text.strip():
        return False
    if "no hay suficiente contexto" in lower or "respuesta basada en el contexto" in lower:
        return False
    return len(text.strip()) > 30

# =============================================================
# ğŸ§  Test principal
# =============================================================
@pytest.mark.asyncio
async def test_natural_queries_extended_v3():
    print("\n==============================")
    print("ğŸ§  TEST AMPLIADO DE PREGUNTAS NATURALES (Gestor de proyectos)")
    print("==============================")

    passed = 0
    for q, expected_keywords in NATURAL_QUERIES:
        print(f"\nğŸ§© Consulta: {q}")
        response = await handlers.handle_query(q)
        lower = response.lower()

        meaningful = _is_meaningful(response)
        has_keyword = _has_keywords(lower, expected_keywords)

        if meaningful and has_keyword:
            print(f"âœ… Respuesta vÃ¡lida ({expected_keywords})")
            passed += 1
        else:
            print(f"âš ï¸ Respuesta dÃ©bil o fuera de contexto: {response[:160]}...")

    ratio = passed / len(NATURAL_QUERIES)
    print(f"\nğŸ“Š Resultado: {passed}/{len(NATURAL_QUERIES)} ({ratio:.0%}) vÃ¡lidas.\n")
    assert ratio >= 0.7  # al menos 70% deben ser coherentes
