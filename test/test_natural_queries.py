#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BaterÃ­a de pruebas con lenguaje natural para validar el comportamiento del Agente Gestor de Proyectos (RAG ClickUp).
Simula cÃ³mo los usuarios harÃ­an preguntas reales al sistema.
"""

import re
import json
from utils.hybrid_search import HybridSearch

# ==============================================================
# CONFIGURACIÃ“N
# ==============================================================

search = HybridSearch(mode="pro")

TEST_QUERIES = [
    # --- Estado de tareas ---
    ("Â¿QuÃ© tareas estÃ¡n bloqueadas?", ["CREAR RAG"]),
    ("Â¿Hay alguna tarea con impedimentos?", ["CREAR RAG"]),
    ("Â¿QuÃ© tareas estÃ¡n en progreso?", []),  # No hay in_progress reales en tu dataset actual
    ("Â¿CuÃ¡les estÃ¡n finalizadas?", ["Titulo tarea", "tarea finalizada", "Test Tarea inicial"]),
    ("Â¿Hay tareas sin empezar?", ["Sin tÃ­tulo", "Titulo tarea"]),
    ("Â¿QuÃ© tareas estÃ¡n pendientes de revisiÃ³n?", []),  # no hay in_review en tu dataset

    # --- Urgentes ---
    ("Â¿QuÃ© tareas son urgentes?", ["tarea finalizada", "Test Tarea inicial"]),

    # --- Por sprint ---
    ("Â¿CuÃ¡ntas tareas hay en el Sprint 1?", ["Sprint 1"]),
    ("MuÃ©strame las tareas del Sprint 2", ["Sprint 2"]),
    ("Â¿QuÃ© hay en el Sprint actual?", ["Sprint 3"]),  # el mÃ¡s alto se considera el actual
    ("Â¿QuÃ© tareas tiene el Sprint 3?", ["Sprint 3"]),

    # --- General ---
    ("Â¿QuÃ© tareas estÃ¡n completadas o cerradas?", ["Titulo tarea", "tarea finalizada", "Test Tarea inicial"]),
    ("Â¿CuÃ¡ntas tareas tenemos en total?", []),
]

# ==============================================================
# FUNCIÃ“N AUXILIAR
# ==============================================================

def normalize(text: str) -> str:
    return re.sub(r"[^a-zÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼0-9 ]", "", text.lower())

# ==============================================================
# EJECUCIÃ“N DE TESTS
# ==============================================================

print("\n==============================")
print("ğŸ” BATERÃA DE PRUEBAS DE LENGUAJE NATURAL")
print("==============================\n")

total = len(TEST_QUERIES)
passed = 0

for query, expected_keywords in TEST_QUERIES:
    print(f"\nğŸ§  Consulta: {query}")
    results = search.search_semantic(query)

    # concatenar textos para verificar presencia de palabras esperadas
    all_docs = " ".join(r.get("text", "").lower() for r in results if "text" in r)


    match_count = 0
    for keyword in expected_keywords:
        if normalize(keyword) in normalize(all_docs):
            match_count += 1

    if expected_keywords and match_count == len(expected_keywords):
        print(f"âœ… Resultado coherente: contiene {expected_keywords}")
        passed += 1
    elif not expected_keywords and len(results) > 0:
        print(f"ğŸŸ¡ No habÃ­a resultado esperado explÃ­cito, pero devolviÃ³ {len(results)} coincidencias.")
        passed += 1
    else:
        print(f"âŒ No se encontraron todas las coincidencias esperadas: {expected_keywords}")
        print("   Resultados obtenidos:")
        for r in results[:3]:
            doc = r.get("text", "")
            print(f" - {doc[:100]}...")

# ==============================================================
# RESUMEN FINAL
# ==============================================================

print("\n==============================")
print("ğŸ“Š RESULTADO FINAL DE PRUEBAS")
print("==============================")
print(f"âœ… {passed}/{total} consultas coherentes ({(passed/total)*100:.1f}%)")
if passed < total:
    print("âš ï¸ Algunas consultas podrÃ­an necesitar mÃ¡s contexto o mejora semÃ¡ntica.")
else:
    print("ğŸ¯ Todas las consultas fueron respondidas de forma coherente.")
